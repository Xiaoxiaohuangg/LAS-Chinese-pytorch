meta_variable:
  experiment_name: 'las_all_datasets'              # Expriment title, log/checkpoint files will be named after this
  checkpoint_dir: 'checkpoint/'               # Folder for model checkpoints, make sure created before running
  training_log_dir: 'log/'                    # Folder for training logs, make sure created before running

model_parameter:
  max_timestep: 2000                          # max_timestep%8 == 0 is required due to listener time resolution reduction
  max_label_len: 100                           # 
  input_feature_dim: 39                       # 
  listener_hidden_dim: 256                    # Default listener LSTM output dimension from LAS paper
  listener_layer: 3                           # Number of layers in listener, the paper is using 3
  multi_head: 1                               # Number of heads for multi-head attention
  decode_mode: 1                              # Decoding mode, 0 : feed char distribution to next timestep, 1: feed argmax, 2: feed sampled vector
  use_mlp_in_attention: True                  # Set to False to exclude phi and psi in attention formula
  mlp_dim_in_attention: 128                   #
  mlp_activate_in_attention: 'relu'           #
  speller_rnn_layer: 1                        # Default RNN layer number 
  speller_hidden_dim: 512                     # Default speller LSTM output dimension from LAS paper
  output_class_dim: 5029                        # 5027 phonemes + 2 for <sos> & <eos>
  rnn_unit: 'LSTM'                            # Default recurrent unit in the original paper
  use_gpu: True
  bucketing: False                            # Bucket training data by input sequence length 
                                              # (Tested and found not improving performance on TIMIT)
  label_smoothing: 0.1                        # Epsilon for label smoothing (set 0 to disable LS)
  data_type: 'float32'
  transform: True 
  target_transform: True

training_parameter:
  learning_rate: 0.0001
  num_epochs: 1000
  seed: 1                  
  total_steps: 100000
  batch_size: 32
  tf_rate_upperbound: 0.8                     # teacher forcing rate during training will be linearly
  tf_rate_lowerbound: 0.0                     # decayinn from upperbound to lower bound for each epoch
  verbose_step: 1                             # Show progress every verbose_step
  valid_step: 100
  use_pretrained: False                       # Load a pretrained model, model path should be given
  pretrained_listener_path: 'checkpoint/las_all_datasets.listener' 
  pretrained_speller_path: 'checkpoint/las_all_datasets.speller'


testing_parameter:
  learning_rate: 0.0001
  seed: 1
  total_steps: 100000
  batch_size: 1
  tf_rate_upperbound: 0.8                     # teacher forcing rate during training will be linearly
  tf_rate_lowerbound: 0.0                     # decayinn from upperbound to lower bound for each epoch
  verbose_step: 5                             # Show progress every verbose_step
  valid_step: 100
  use_pretrained: True                       # Load a pretrained model, model path should be given
  pretrained_listener_path: 'checkpoint/las_all_datasets.listener'
  pretrained_speller_path: 'checkpoint/las_all_datasets.speller'

